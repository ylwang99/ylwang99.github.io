
<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Course Page -- Algorithms for Data Science (CMSC 644/CMSC 498U)</title>


    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/resume.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#overview">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#readings">Readings</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#homeworks">Homeworks</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#schedule_old">Schedule</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.umd.edu/web-accessibility" title="UMD Web
                      Accessibility">Web Accessibility</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="overview">
        <div class="my-auto">
            <h2 class="mb-1" >
                Algorithms for Data Science (CMSC 644/498U)
            </h2>
            <div class="mb-5 ml-3">
                <p>
                    <strong>Class Time:</strong> Mon 7:00pm--9:30pm. CSI 3120.
                </p>
                <p>
                    <strong>Overview:</strong> As large amounts of data are
                    being created it is important to understand how to analyse
                    the data to extract interesting trends and patterns. Since
                    the volume of data is large, it may not be feasible to make
                    more than a single pass over the data. Stream processing
                    methods provide effective ways to extract useful information
                    from large data sets by making very few passes on the data.
                    Surprisingly, a lot of information can be gleaned by making
                    a single pass over the data, or a small number of passes
                    over the data. The first part of the course will cover
                    random sampling and stream processing methods. We will also
                    consider privacy issues in data bases and how these should
                    be handled.
                </p>
                <p>
                    <strong>Course Work:</strong> Course work will consist of homeworks and
                    two exams. The relative weights of these will be 30% for the homeworks,
                    30% for the midterm and 40% for the final exam.
                </p>
                <p>
                    <strong>Prerequisites:</strong> CMSC 351. I expect
                    familiarity with basic algorithms. This course is an
                    algorithmic oriented course, with proofs of correctness etc.
                </p>
            </div>
          <h3 class="mb-0">Instructor:
            <a href="http://www.cs.umd.edu/users/samir/">Samir Khuller</a>
          </h3>
          <div class="mb-1 ml-3">
              <div>
                  <strong>Office</strong>: AVW 3369.
                  <strong>Office phone</strong>: (301) 405--6765.
              </div>
              <a href="mailto:samir@cs.umd.edu">samir@cs.umd.edu</a>
              <p>
                  <strong>Office Hours:</strong> Mon 5:30pm-6:45pm. If you cannot make
                  these hours, please make an appointment to see me at a
                  different time.
              </p>
          </div>
          <h3 class="mb-0">Teaching Assistant:
            <a href="http://www.cs.umd.edu/~styang/">Sheng Yang</a>
          </h3>
          <div class="mb-1 ml-3">
              <div>
                  <strong>Office</strong>: AVW 3457.
              </div>
              <a href="mailto:styang@cs.umd.edu">styang@cs.umd.edu</a>
              <p>
                  <strong>Office Hours:</strong> TuTh 4:00pm-5:00pm. If you cannot make
                  these hours, please make an appointment to see me at a
                  different time.
              </p>
          </div>
          <div class="mt-4">
              <p>
                  <em>I will update this page every week during the semester. I
                      will place all homeworks as well as solutions to homeworks
                      here. If you have any trouble accessing them, please let me
                      know.
              </em>
              </p>
          </div>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="readings">
        <div class="my-auto">
          <h2 class="mb-5">Readings</h2>

          <div class="ml-3">
              <h3>Main Textbook:</h3>
              <div class="ml-2">
                  <ul>
                      <li>
                  <a href="stanford_book.pdf">Mining of Massive Datasets</a> by Jure Leskovec, Anand Rajaraman, Jeff Ullman. We will be mainly using this book
                      </li>
                  </ul>
              </div>
          </div>
          <div class="ml-3">
              <h3>Useful Readings:</h3>
              <div class="ml-2">
                  <ul>
                      <li>
                          <a href="cornell_book.pdf">Fundations of Data Science</a> by Avrim Blum, John Hopcroft, and Ravindran Kannan.
                      </li>
                      <li>
                          <a href="https://www.cs.rutgers.edu/%7Emuthu/streams.html">Data Streaming</a> by S. Muthukrishnan.
                      </li>
                  </ul>
              </div>
          </div>
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="homeworks">
        <div class="my-auto">
          <h2 class="mb-5">Homeworks</h2>

          <div class="ml-5" >
              <li>Homework 1: <a href="homework/hw3.pdf">link</a>. <a href="homework/input.zip">Input files</a>. Due date: 2/14, 5 p.m.</li>
              <li>Homework 2: <a href="homework/hw3.pdf">link</a>. <a href="homework/input_2.zip">Input files</a>. Due date: 03/01, 5p.m.</li>
              <li>Homework 3: <a href="homework/hw3.pdf">link</a>. Due date: 03/27, 5p.m.</li>
              <li>Homework 4 (the same as midterm, will take the maximum of midterm and homework 4 as score): <a href="homework/hw4.pdf">link</a>. Due date: 04/17, 5p.m.</li>
              <li>Homework 5: <a href="homework/hw5.pdf">link</a>. <a href="homework/input_5.zip">Input files</a>. Due date: 05/02, 5p.m.</li>
          </div>
        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="schedule_old">
        <div class="my-auto mb-5">
          <h2 class="mb-1">Schedule</h2>
        </div>
        <ul>
            <li>
                <a href="lecture_notes/1.pdf">Lecture 1</a> (Jan 29): Overview, data analysis, algorithms, Sampling
            </li>
            <li>
                <a href="lecture_notes/2.pdf">Lecture 2</a> (Feb 5): Bonferroni Principle, Sampling, Hash function [Chapter 4]
            </li>
            <li>
                <a href="lecture_notes/3.pdf">Lecture 3</a> (Feb 12): Streaming, Frequency Estimation, Distinct Element Estimation, <a href="karp.pdf">Finding Frequent Elements</a> [Chapter 4]
            </li>
            <li>
                <a href="lecture_notes/4.pdf">Lecture 4</a> (Feb 19): K-center, and guest speaker Prof. David Mount covered Coresets. [Chapter 5]
            </li>
            <li>
                <a href="lecture_notes/5.pdf">Lecture 5</a> (Feb 26): Linear Programming, <a href="CCFM.pdf">Incremental K-center</a>
            </li>
            <li>
                <a href="lecture_notes/6.pdf">Lecture 6</a> (March 5): Guest lecture by Dr. Jessica on Linear Programming and Gurobi.
                <a href="gurobi_files.zip">
                    Files to be used
                </a>
            </li>
            <li>
                <a href="lecture_notes/7.pdf">Lecture 7</a> (March 12): K-shingles, min-hash [Chapter 3]
            </li>
            <li>
                <a href="lecture_notes/8.pdf">Lecture 8</a> (March 26): LSH, Link Analysis [Chapter 3, 5]
            </li>
            <li>
                Lecture 9 (April 2): Information Visualization (Ben Shneiderman), midterm
            </li>
            <li>
                <a href="lecture_notes/10.pdf">Lecture 10 (April 9)</a>: Social Network Analysis [Chapter 10?]
            </li>
            <li>
                Lecture 11 (April 16): Guest lecture by Dr. Jessica, topic TBD
            </li>
            <li>
                Lecture 12 (April 23): TBD
            </li>
            <li>
                Lecture 13 (April 30): TBD
            </li>
            <li>
                Lecture 14 (May 7): Final exam
            </li>
        </ul>
        <div class="my-auto mb-5">
          <h2 class="mb-1">Previous Schedule</h2>
          <p>
              Here is the schudule of some previous term. This gives some ideas about the course, but the material for this term will <strong>NOT</strong> be the same!
          </p>
        </div>
        <ul>
            <li>Lecture 1 (Jan 29): Overview of the course. Finding
                Min/Max with (3/2) n comparisons. Discussion of lower bounds.
                Discussion of Divide and Conquer based Selection. READ: Intro
                to Algorithms, by Cormen, Leiserson, Rivest and Stein (Chap
                9).
            </li>
            <li>Lecture 2 (Jan 31): Streaming <a href= "munro-paterson.pdf">algorithm</a> for selection by Munro and
                Paterson.
            </li>
            <li>Lecture 3 (Feb 5): More on selection (Munro-Paterson).
                See handout as well. Simple sampling based selection
                algorithm also covered.
            </li>
            <li>Lecture 4 (Feb 7): Start on the Manku-Rajagopalan-Lindsay
                <a href="manku.pdf">paper</a> for approximate selection in a
                single pass. Will distribute <a href="kh.pdf">for other
                    material</a> as well.
            </li>
            <li>Lecture 5 (Feb 12): Random Sampling and <a href= "vitter.pdf">Reservoir Sampling</a>.
            </li>
            <li>Lecture 6 (Feb 14): More on Sampling (see notes),
                especially Markov, and Chernoff bounds and sample size
                required to estimate number of blue balls in a bin and Bloom
                Filters.
            </li>
            <li>Lecture 7 (Feb 19): Finding heavy hitters (see <a href=
                                                                  "mitup.pdf">handout</a>).
            </li>
            <li>Lecture 8 (Feb 21): Finish proof from last time and how
                do we prove correctness of algorithms?
            </li>
            <li>Lecture 9 (Feb 26): <a href="karp.pdf">Finding Frequent
                elements (2 pass scheme)</a> and amortized analysis.
            </li>
            <li>Lecture 10 (Feb 28): Hash functions
            </li>
            <li>Lecture 11 (Mar 4): <a href="triang.pdf">Triangle
                counting in streams</a>.
            </li>
            <li>Lecture 12 (Mar 6): <a href="distinct.ps">Counting
                distinct elements</a>. <a href="lec12.pdf">NOTES</a>
            </li>
            <li>Lecture 13 (Mar 11): Review?
            </li>
            <li>Lecture 14 (Mar 13): MIDTERM (Closed Notes/Closed Book)
            </li>
            <li>Lecture 15 (Mar 25): <a href=
                                        "http://www.cs.princeton.edu/~moses/papers/incremental_clustering.ps">
                Clustering of Streaming Data</a> (Lecture by Matt
                McCutchen)
            </li>
            <li>Lecture 16 (Mar 27): <a href=
                                        "http://www.cis.upenn.edu/~sudipto/mypapers/histjour.pdf">Data
                streams and histograms</a>.
            </li>
            <li>Lecture 17 (Apr 1): Mainly discussed possible class
                presentations and some stuff on Locally Sensitive Hashing and
                Voronoi Diagrams
            </li>
            <li>Lecture 18 (Apr 3): Data Layout Problems
            </li>
            <li>Lecture 19 (Apr 8): <a href="SVM.pdf">Support Vector
                Machines</a> and <a href="PCA.pdf">Principal Component
                Analysis</a> (Guest lecture by Dr. Rezarta Islamaj)
            </li>
            <li>Lecture 20 (Apr 10): <a href=
                                        "http://www.cs.umd.edu/~varshney/4samir">Data
                Visualization</a> (Guest lecture by Prof. Varshney)
            </li>
            <li>Lecture 21 (Apr 15): <a href="10Algorithms-08.pdf">Data
                Mining (kNN and Classification)</a>
            </li>
            <li>Lecture 22 (Apr 17): <a href="interface.pdf">Data Mining
                (More on kNN approaches)</a>
            </li>
            <li>Lecture 23 (Apr 22): Noah Easterly (Frequent Items/Assoc
                Rules) <a href="6asso.ppt">Slides</a> <a href=
                                                         "zaki02charm.pdf">Zaki paper</a> <a href="wang03closet.pdf">
                Wang paper</a> Matt McCutchen (Clustering with Outliers)
                <a href="clustering.pdf">Clustering paper</a>
            </li>
            <li>Lecture 24 (Apr 24): Sharath Srinivas (Top K monitoring)
                <a href="topk.pdf">Slides</a> Aaron Cordova <a href=
                                                               "http://www.ams.org/featurecolumn/archive/pagerank.html">(Page
                    Rank</a> and Map Reduce)
            </li>
            <li>Lecture 25 (Apr 29): Dave Malhotra (Decision Trees)
                <a href="Decision_Trees.pptx">Slides (DM)</a> Rosa Cowan
                (Predictive Classifiers Chapter 10) <a href="rosa.ppt">Slides
                    (RC)</a>
            </li>
            <li>Lecture 26 (May 1): <a href=
                                       "http://www.cs.purdue.edu/~jsv/Papers/Vit.IO_survey.pdf">I/O
                Eff. Algs.</a> Nick Kuilema (Chap 12) &amp; Antoni Gmurczyk
                (Chap 10) <a href="multid_data_structures.pdf">Slides
                (NK)</a> <a href="antoni.pptx">Slides (AG)</a>
            </li>
            <li>Lecture 27 (May 6): Marianna Martindale (Statistical
                Machine Translation 1) (Statistical
                Machine Translation 2) <a href=
                                          "statmt-cmsc498k.ppt">(Slides (MM))</a> Hyoungtae Cho
                <a href="Amazon-Recommendations.pdf">(Collaborative
                    Filtering 1)</a> <a href=
                                        "schafer01ecommerce.pdf">(Collaborative Filtering 2)</a>
                <a href="CMSC498K_Hyoungtae_Cho.ppt">Slides (HC)</a>
            </li>
            <li>Lecture 28 (May 8): Martin Paraskevov (Locally Sensitive
                Hashing) Rustin Rockstroh (Fraud Detection)
            </li>
            <li>Lecture 29 (May 13): Tim Creech &amp; Charles Hawkins
                (Karp's random streaming model)
            </li>
        </ul>
      </section>

    </div>


    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>

  </body>

</html>

